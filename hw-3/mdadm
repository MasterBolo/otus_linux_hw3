
                                      Домашнее задание

                                       Работа с mdadm

 Цель работы: научиться использовать утилиту для управления программными RAID-массивами в Linux.

 Для выполнения домашнего задания использовать прилагаемую методичку.

 Что нужно сделать?

    *добавить в Vagrantfile еще дисков;
    *сломать/починить raid;
    *собрать R0/R5/R10 на выбор;
    *прописать собранный рейд в конф, чтобы рейд собирался при загрузке;
    *создать GPT раздел и 5 партиций.

 На проверку отправить

    *измененный Vagrantfile,
    *скрипт для создания рейда,
    *конф для автосборки рейда при загрузке.

 Доп. задание*

 Vagrantfile, который сразу собирает систему с подключенным рейдом и смонтированными разделами. После перезагрузки стенда разделы должны  автоматически  примонтироваться.

Задание повышенной сложности**

перенести работающую систему с одним диском на RAID 1. Даунтайм на загрузку с нового диска предполагается.

					    Выполнение


	*Добавление в Vagrantfile еще дисков

 Создаю в домашней директории Vagrantfile, в тело данного файла копирую содержимое Vagrantfile репозитория git-a: https://github.com/erlong15/otus-linux.
Для того, чтобы наглядно увидеть процесс rebuild-a для создаваемого RAID, изменим размеры диского пространства для каждого блока с 250 до 1000 Mb.
В исходный листинг, добавляю блок, описывающий 5-й номер порта sata и подключаемый диск (Листинг 1.1)

				(Листинг 1.1 - добавление жесткого диска)


		:sata5 => {
                        :dfile => './sata5.vdi',
                        :size => 1000, # Megabytes
                        :port => 5
		}

 Собираю стенд командой: - [nur@test hw-3]$ vagrant up
 Подключаюсь к стенду: -[nur@test hw-3]$ vagrant ssh

 Посмотрим какие блочные устройства имеются в системе:

 [root@otuslinux vagrant]# lshw -short | grep disk
/0/100/1.1/0.0.0    /dev/sda  disk        42GB VBOX HARDDISK
/0/100/d/0          /dev/sdb  disk        1048MB VBOX HARDDISK
/0/100/d/1          /dev/sdc  disk        1048MB VBOX HARDDISK
/0/100/d/2          /dev/sdd  disk        1048MB VBOX HARDDISK
/0/100/d/3          /dev/sde  disk        1048MB VBOX HARDDISK
/0/100/d/0.0.0      /dev/sdf  disk        1048MB VBOX HARDDISK

 Стенд имеет шесть блочных устройств : sda-sdf. Создадим R10.

	*Создание RAID 10-го уровня


 Создадим R10, включающий четыре диска - sdb,sdc,sdd,sde:

 [root@otuslinux vagrant]# mdadm --create --verbose /dev/md1 -l 10 -n 4 /dev/sd{b,c,d,e}
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: size set to 1021952K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.

 Проверим созданный RAID командой:

 [root@otuslinux vagrant]# cat /proc/mdstat
Personalities : [raid10] 
md1 : active raid10 sde[3] sdd[2] sdc[1] sdb[0]
      2043904 blocks super 1.2 512K chunks 2 near-copies [4/4] [UUUU]
      
unused devices: <none>

	*Создание файла конфигурации RAID

 Для того, чтобы стенд подключал текущую конфигурацию созданного RAID-а после перезагрузки,создадим
дирректорию mdadm и файл в данной дирректории mdadm.conf:
 [root@otuslinux vagrant]# mkdir /mdadm
 [root@otuslinux vagrant]# echo "DEVICE partitions" > /mdadm/mdadm.conf
 [root@otuslinux vagrant]# mdadm --detail --scan --verbose | awk '/ARRAY/{print}' >> /mdadm/mdadm.conf


	*Выводим из строя/ремонтируем созданный RAID

 Выводим из строя RAID с помощью команды:

 [root@otuslinux vagrant]# mdadm /dev/md1 --fail /dev/sde
mdadm: set /dev/sde faulty in /dev/md1

 Посмотрим текущее состояние RAID:

 [root@otuslinux vagrant]# mdadm -D /dev/md1 
/dev/md1:
           Version : 1.2
     Creation Time : Sat Feb  3 14:09:35 2024
        Raid Level : raid10
        Array Size : 2043904 (1996.00 MiB 2092.96 MB)
     Used Dev Size : 1021952 (998.00 MiB 1046.48 MB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent
       Update Time : Sat Feb  3 14:43:35 2024
             State : clean, degraded 
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 1
     Spare Devices : 0
            Layout : near=2
        Chunk Size : 512K
Consistency Policy : resync
              Name : otuslinux:1  (local to host otuslinux)
              UUID : 47b22f0d:2b587387:50bdb534:af04d295
            Events : 19
    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync set-A   /dev/sdb
       1       8       32        1      active sync set-B   /dev/sdc
       2       8       48        2      active sync set-A   /dev/sdd
       -       0        0        3      removed

       3       8       64        -      faulty   /dev/sde

 Из отчёта мы наблюдаем, что RAID "md1" перешёл в состояние "degraded", диск "sde" в состоянии "faulty" и требует замены
 на исправный диск.

 Заменим неисправный диск sde на исправный sdf.

 Удалим неисправный диск sde с помощью команды:
 [root@otuslinux vagrant]# mdadm /dev/md1 --remove /dev/sde
mdadm: hot removed /dev/sde from /dev/md1

 Добавим исправный диск sdf в RAID коммандой:
 [root@otuslinux vagrant]# mdadm /dev/md1 --add /dev/sdf
mdadm: added /dev/sdf

 Выведем отчёт по RAID:
[root@otuslinux vagrant]# mdadm -D /dev/md1
/dev/md1:
           Version : 1.2
     Creation Time : Sat Feb  3 14:09:35 2024
        Raid Level : raid10
        Array Size : 2043904 (1996.00 MiB 2092.96 MB)
     Used Dev Size : 1021952 (998.00 MiB 1046.48 MB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent
       Update Time : Sat Feb  3 15:08:04 2024
             State : clean 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0
            Layout : near=2
        Chunk Size : 512K
Consistency Policy : resync
              Name : otuslinux:1  (local to host otuslinux)
              UUID : 47b22f0d:2b587387:50bdb534:af04d295
            Events : 39
    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync set-A   /dev/sdb
       1       8       32        1      active sync set-B   /dev/sdc
       2       8       48        2      active sync set-A   /dev/sdd
       4       8       80        3      active sync set-B   /dev/sdf

 
Из отчёта мы наблюдаем, что состояние "degraded" - исчезло , диск "sdf" прошёл ребилд и находится в состоянии "active". 

RAID md1 - отремонтирован.

	
	*Создание GPT раздела и 5 партиций

 Создадим GPT раздел на RAID на md1 с помощью команды:
[root@otuslinux vagrant]# parted -s /dev/md1 mklabel gpt

 Создадим партиции с помощью команды:
[root@otuslinux vagrant]# parted /dev/md1 mkpart primary ext4 0% 20%
Information: You may need to update /etc/fstab.

[root@otuslinux vagrant]# parted /dev/md1 mkpart primary ext4 20% 40%     
Information: You may need to update /etc/fstab.

[root@otuslinux vagrant]# parted /dev/md1 mkpart primary ext4 40% 60%    
Information: You may need to update /etc/fstab.

[root@otuslinux vagrant]# parted /dev/md1 mkpart primary ext4 60% 80%    
Information: You may need to update /etc/fstab.

[root@otuslinux vagrant]# parted /dev/md1 mkpart primary ext4 80% 100%   
Information: You may need to update /etc/fstab.

 Cоздадим на партициях файловую систему с помощью команды:
[root@otuslinux vagrant]# for i in $(seq 1 5); do sudo mkfs.ext4 /dev/md1p$i; done
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=512 blocks, Stripe width=1024 blocks
102000 inodes, 407552 blocks
20377 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=34078720
50 block groups
8192 blocks per group, 8192 fragments per group
2040 inodes per group
Superblock backups stored on blocks: 
	8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

 После выполнения команды, наблюдаем пять блоков отчёта для пяти созданных партиции.

 Монтируем созданные файловые системы по каталогам с помощью команд:
[root@otuslinux vagrant]# mkdir -p /raid/part{1,2,3,4,5}
[root@otuslinux vagrant]# for i in $(seq 1 5); do mount /dev/md1p$i /raid/part$i; done

 Посмотрим созданные каталоги на RAID md1 c помощью команды:
[root@otuslinux vagrant]# fdisk -l

WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.

Disk /dev/md1: 2092 MB, 2092957696 bytes, 4087808 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 524288 bytes / 1048576 bytes
Disk label type: gpt
Disk identifier: E98A835F-077C-4F96-AD6C-6725B20C24B2


#         Start          End    Size  Type            Name
 1         2048       817151    398M  Microsoft basic primary
 2       817152      1634303    399M  Microsoft basic primary
 3      1634304      2453503    400M  Microsoft basic primary
 4      2453504      3270655    399M  Microsoft basic primary
 5      3270656      4085759    398M  Microsoft basic primary

  Из отчёта наблюдаем созданный RAID md1 размером 2 Гигабайта с таблицей разделов gpt и созданные
пять каталогов с файловой системой ext4. 

	Доп. задание*

  Создать Vagrantfile, который сразу собирает систему с подключенным рейдом и смонтированными разделами. После перезагрузки стенда разделы должны  автоматически  примонтироваться.

  Для выполнения данной задачи, в Vagrantfile, в разделе "box.vm.provision "shell", inline: <<-SHELL"
неоходимо добавить листинг со следующим содержимым (Листинг 1.2):
				
				(Листинг 1.2 - Скрипт для создания и конфигурирования RAID)

 sudo mdadm --create --verbose /dev/md1 -l 10 -n 4 /dev/sd{b,c,d,e}
 sudo mkdir /mdadm
 sudo echo "DEVICE partitions" > /mdadm/mdadm.conf
 sudo mdadm --detail --scan --verbose | awk '/ARRAY/{print}' >> /mdadm/mdadm.conf
 sudo parted -s /dev/md1 mklabel gpt
 sudo parted /dev/md1 mkpart primary ext4 0% 20%
 sudo parted /dev/md1 mkpart primary ext4 20% 40%
 sudo parted /dev/md1 mkpart primary ext4 40% 60%
 sudo parted /dev/md1 mkpart primary ext4 60% 80%
 sudo parted /dev/md1 mkpart primary ext4 80% 100%
 for i in $(seq 1 5); do sudo mkfs.ext4 /dev/md1p$i; done
 sudo mkdir -p /raid/part{1,2,3,4,5}
 for i in $(seq 1 5); do mount /dev/md1p$i /raid/part$i; done

 После выполнения Vagrantfile, получим стенд с уже подключенным RAID-10, с таблицей разделов gpt и каталогами.









 
 

	


 

	   

 


